---
title: "Nini Text Analysis"
author: "Rob Wells"
date: "2025-02-05"
output: html_document
---

Independent study with Nini Mtchedlishvili

This code demonstrates text processing, cleaning and the creation of bigrams and sentiment analysis from a group of Daily Wire news articles about Kamala Harris and Donald Trump.

**Credits:**

-   Julia Silge & David Robinson, Text Mining with R: <https://www.tidytextmining.com/>

-   Nino Mtchedlishvili, Ph.D. student at the Philip Merrill College of Journalism, contributed to this project.







The analysis of The Daily Wire’s coverage of Kamala Harris and Donald Trump during October and early November 2024 reveals notable differences in framing, sentiment, and linguistic choices. By examining bigrams, sentiment scores, and headline sentiment, as well as adjective-noun pairs, the data presents a distinct pattern in how each candidate was portrayed. While both Harris and Trump received significant negative coverage, the nature and framing of this negativity varied considerably, highlighting the publication’s ideological stance and strategic editorial choices.

A close look at bigrams—two-word phrases frequently appearing together—provides insight into the main themes surrounding each candidate. For Harris, "Kamala Harris" was the most common bigram, appearing 242 times, followed by "vice president" (109) and "Donald Trump" (98). Other frequently mentioned phrases included "president Kamala" (81), "Biden Harris" (62), and "Harris campaign" (55). The presence of "Matt Walsh’s" (51) and "Daily Wire" (50) suggests a strong involvement of the platform’s main commentators in discussions about Harris. These results indicate that Harris was primarily discussed in the context of her position as vice president and her presidential campaign. However, the consistent pairing of her name with Trump’s suggests that she was often framed in direct comparison to him, rather than discussed as an independent political figure.

Trump’s bigrams reflect a similar trend, though with some key differences. "Donald Trump" appeared 198 times, closely followed by "Kamala Harris" (184), which underscores how the two were frequently discussed together. Other high-frequency phrases included "president Donald" (88), "president Trump" (65), "election day" (63), and "Trump campaign" (40). The overlap in key terms, particularly "president Kamala" (58) and "Harris campaign" (38), suggests that Harris was often framed as Trump’s direct rival rather than being analyzed on her own merits. While both were discussed in the context of the election, Trump’s bigrams emphasized his role as president or candidate, whereas Harris’s included phrases that framed her candidacy in a way that underscored uncertainty or contrast.

Sentiment analysis further highlights disparities in coverage. The sentiment scores for Harris skewed overwhelmingly negative, with a significant number of articles displaying sharp declines in sentiment scores. Few articles contained strongly positive sentiment toward Harris, reinforcing the conclusion that The Daily Wire maintained a consistently critical tone in its coverage. The negativity appeared sustained over time, with little fluctuation. Trump’s sentiment distribution, however, showed more variation. While he also received negative coverage, there were notable spikes in positive sentiment, indicating moments when The Daily Wire framed him favorably or defended him against criticism. This difference is significant because it suggests that Harris was almost exclusively portrayed in a negative light, whereas Trump was given some space for praise, contextualizing his negative coverage as a response to external challenges rather than inherent faults.

A timeline comparison of sentiment scores for Harris and Trump from October 14 to November 4, 2024, reveals further disparities. Harris’s coverage remained predominantly negative, with only brief instances of neutral or slightly positive sentiment. In contrast, Trump’s sentiment showed greater fluctuation, with certain days exhibiting overwhelmingly positive sentiment. This pattern suggests that The Daily Wire selectively framed Trump’s coverage to include moments of vindication or approval, while Harris’s coverage remained consistently critical. The data also indicates that Harris received a relatively higher proportion of extremely negative sentiment scores compared to Trump, reinforcing the outlet’s harsher tone in discussing her candidacy.

The sentiment expressed in article headlines also aligns with these trends. Headlines about Harris were consistently more negative than those about Trump, with fewer instances of neutrality or positivity. Since headlines serve as the primary entry point for readers, this suggests an intentional framing strategy designed to prime audiences for a negative perception of Harris before they even engage with the full content of the articles. By contrast, Trump’s headlines, while often critical, included a mix of frames that portrayed him as a political figure under attack, reinforcing a narrative of persecution rather than outright failure.

An analysis of the most frequently occurring negative bigrams for both candidates provides additional context. Harris was frequently associated with terms such as "violent crime," "criminal illegal," "dumb evil," "damn shame," and "felony charges." These phrases reflect a clear attempt to align her with criminality, weakness, and incompetence—consistent with broader conservative narratives that criticize Democratic leaders for being ineffective or soft on crime. The frequent repetition of crime-related terms in discussions about Harris suggests a deliberate effort to invoke negative connotations and instill skepticism about her leadership capabilities.

Trump’s negative bigrams, on the other hand, had a different thematic focus. While he was also associated with crime-related terms, these terms often appeared in contexts suggesting unfair treatment rather than inherent criminality. Phrases like "fake charges," "lawsuits fake," "severe trauma," and "death penalty" indicate that negative coverage of Trump frequently revolved around allegations and legal battles rather than personal shortcomings. This contrast is crucial because it suggests that while Harris was framed as an actively harmful or incompetent leader, Trump’s negative coverage was shaped around the idea of external forces working against him. This framing aligns with common right-wing narratives that depict Trump as a victim of political persecution rather than a flawed leader.

The comparative analysis of adjective-noun pairs further supports these findings. Harris’s most frequently associated adjectives—such as "radical," "failed," and "dangerous"—paint her as an extreme, ineffective, or risky candidate. In contrast, Trump’s most common adjective-noun pairings often involved words like "fake," "unfair," or "strong," suggesting that his coverage was at times critical but still provided room for resilience and defense against external accusations.

While both Harris and Trump were discussed in largely negative terms, the framing of their negativity was fundamentally different. Harris was portrayed as an inherently flawed or incapable leader, while Trump was often framed as a controversial but embattled figure. This difference in treatment reflects The Daily Wire’s ideological leanings and its strategic approach to influencing audience perceptions.

Despite the depth of these findings, there are limitations to the analysis. Sentiment analysis using the Afinn lexicon, while useful for gauging overall tone, does not account for context, irony, rhetorical nuances, or inflammatory wording that might alter meaning. Additionally, bigram and adjective-noun pair analysis captures linguistic associations but does not provide full insight into the surrounding discourse. For example, while "strong leadership" might appear frequently in discussions about Trump, the phrase could be used in both positive and negative contexts. Similarly, "president Kamala" might be used in discussions about her qualifications or as a sarcastic critique of her potential presidency. Another limitation is that this analysis focuses solely on The Daily Wire and does not compare its coverage to other media outlets, making it difficult to determine whether these trends are unique to conservative media or reflective of broader partisan coverage patterns.

Ultimately, the findings indicate that The Daily Wire consistently framed Harris in a more negative light than Trump, employing linguistic choices that reinforced perceptions of incompetence, radicalism, and weakness. Trump, while also subject to criticism, was often framed in a way that emphasized external threats and unfair treatment rather than personal failings. The overlap in discussion topics suggests that Harris and Trump were frequently mentioned in relation to each other, reinforcing a binary election narrative. The use of negative crime-related terminology in discussions of Harris, contrasted with the victimization narrative surrounding Trump, further underscores how media framing can shape public perceptions in subtle but significant ways.


### Load software

```{r message=FALSE, warning=FALSE}
#Install packages if you don't have them already
#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("DT")

library(tidyverse)
library(tidytext) 
library(quanteda)
library(readtext)
library(DT)

```

### Harris text

```{r}

files <- list.files("./kamala_extracted_text", pattern="*.txt") |> 
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(name = str_replace_all(filename, ".txt", "")) |> 
  mutate(name = str_replace_all(tolower(name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number())
         
          
 #  #create an index with the file name
 # mutate(index = str_extract(filename, "\\d+")) |> 
 #  mutate(index = as.numeric(index))

kharris_index <- rio::import("DW_Kamala_Articles.xlsx") |> 
  mutate(name = str_replace_all(tolower(NAME), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number()) |> 
  distinct(name, .keep_all = TRUE) |> 
  mutate(DATE = case_when(
    str_detect(as.character(DATE), "2023-11-04") ~ as_datetime("2024-11-04"),
    TRUE ~ DATE
    ))
```

```{r}
final_index <- files |> 
  inner_join(kharris_index, by=c("name")) |> 
  mutate(filepath = paste0("./kamala_extracted_text/", filename)) |> 
  janitor::clean_names() |> 
  rename(index = index_x) |> 
  distinct(index, .keep_all = TRUE) |> 
  select(index, date,filename,url, filepath, filename)

#fact check
anti <- files |> 
  anti_join(kharris_index, by=c("name"))

```

**don't need to run this skip to line 163**
### Text compiler

```{r include=FALSE, echo=FALSE}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index |>
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) |>
    as_tibble() |>
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df |>
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df |>
  select(filename, sentence=value) |>
  inner_join(final_index)

```

## Clean data

```{r}
phrases <- c("Date:" = "", 
            "Article Text:" = "",
            "Headline:" = "",
            "Already have an account?" = "", 
            "Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
            "Stay up-to-date on the latestnews, podcasts, and more." = "",
  "https" = "")


harris_articles_df <- articles_df |> 
   mutate(sentence = str_replace_all(sentence, phrases))

#write.csv(harris_articles_df, "kharris_extracted_text_jan2025.csv")
```

# ---------------------------

# Load harris_articles_df data

# ---------------------------

```{r}
harris_articles_df <- read.csv("kharris_extracted_text_jan2025.csv")
```

fact check

```{r}
harris_articles_df |> 
  distinct(filename) |> 
  count(filename) |> 
  summarise(sum(n))

```

### Harris Bigrams - Top Phrases

```{r}
harris_bigrams <- harris_articles_df |> 
  select(sentence) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |> 
  filter(!is.na(word1))

head(harris_bigrams)
```

```{r include=FALSE}

top_20_harris_bigrams <- harris_bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```

```{r}
datatable(top_20_harris_bigrams,
          caption = "Top 20 Harris Phrases",
          options = list(pageLength = 20)) 
```

## Chart Top Harris Phrases

```{r}
ggplot(top_20_harris_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Kamala Harris on daily Wire coverage",
       caption = "n=94 articles, 2024. Graphic by Rob Wells. 1-23-2025",
       x = "Phrase",
       y = "Count of terms")
```

### Measure Sentiment of Harris articles

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
harris_sentiment <- harris_articles_df |> 
  select(sentence, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# join tokenized words with the sentiment dictionary
harris_sentiment_analysis <- harris_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index) |>  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |> 
  inner_join(final_index, by=("index")) |> 
  select(index, date, sentiment, sentiment_type, filename, url)
```

## Chart sentiment by article

```{r warning=FALSE}

ggplot(harris_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of Daily Wire coverage of Kamala Harris, October 2024",
       caption = "n=96 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Articles",
       y = "Sentiment Score") 
```
# ---------------------------

Trump


```{r}

trump_articles_df <- read_csv("trump_extracted_text_jan2025.csv")


```

```{r}

trump_articles_df |>
  distinct(filename) |>
  count(filename) |>
  summarise(sum(n))

```

```{r}

trump_bigrams <- trump_articles_df |> 
  select(sentence) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |> 
  filter(!is.na(word1))

head(trump_bigrams)


```
```{r include=FALSE}

top_20_trump_bigrams <- trump_bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```

```{r}
datatable(top_20_trump_bigrams,
          caption = "Top 20 Trump Phrases",
          options = list(pageLength = 20))

```

```{r}
ggplot(top_20_trump_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Donald Trump on Daily Wire coverage",
       caption = "n=94 articles, 2024. Graphic by Nini Mtchedlishvili. 1-23-2025",
       x = "Phrase",
       y = "Count of terms")
```


```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df |> 
  filter(!is.na(sentence)) |> #needed to filter na value
  select(sentence, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# join tokenized words with the sentiment dictionary
trump_sentiment_analysis <- trump_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index) |>  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |> 
  inner_join(final_index, by=("index")) |> 
  select(index, date, sentiment, sentiment_type, filename, url)

```

## Chart sentiment by article

```{r warning=FALSE}

ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
       caption = "n=96 articles. Afinn sentiment. Graphic by Nini Mtchedlishvili 1-25-2025",
       x = "Articles",
       y = "Sentiment Score") 

```


# ---------------------------

# Analyze Trump and Harris Articles

# ---------------------------

```{r}
combined <- read.csv("trump-harris-DW-combined.csv")

#There are nine common articles the Trump / Harris corpra. Create a new category for shared article
combined_index <- combined |>
  distinct(filename, url, candidate, .keep_all = TRUE) |>
  group_by(filename) |>
  mutate(shared_article = n() > 1) |>
  ungroup()

#write.csv(combined_index, "combined_index.csv")

```

fact check

```{r}
combined |> 
  distinct(filename, url, candidate) |>
  count(candidate)
```

### Sentiment of Daily News coverage

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
combined_sentiment <- combined |> 
  select(sentence, candidate, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index, candidate) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
combined_sentiment_analysis <- combined_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index, candidate) |>  
  summarize(sentiment = sum(value), .groups = "drop") |> 
  inner_join(combined_index, by = c("index", "candidate"))

# aggregate at article level, total sentiment score (Positive or Negative)
combined_sentiment_analysis <- combined_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) 

```

### Chart Trump, Harris Sentiment by Date

```{r}

ggplot(combined_sentiment_analysis, 
       aes(x = date, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
  labs(title = "Harris, Trump Sentiment in Daily Wire coverage",
       caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
```

# Examine Top Negative Phrases

```{r}
negative_bigrams <- combined |> 
  select(sentence, candidate) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  #adding candidate here to specify Trump v Harris
  count(candidate, word1, word2, sort = TRUE) |> 
  filter(!is.na(word1)) |> 
  mutate(bigram = paste0(word1," ", word2))|> 
  #left join keeps all scores, need to do it twice
   left_join(afinn, by = c("word1" = "word")) |>
  rename(value1 = value) |>
  # left join second word
  left_join(afinn, by = c("word2" = "word")) |>
  rename(value2 = value) |>
  # Combine scores
  mutate(
    total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
  ) |> 
  filter(total_sentiment < 0)
  
```

```{r}
top_50_negative_bigrams <- negative_bigrams |> 
  slice_min(total_sentiment, n= 50) |> 
  select(bigram, total_sentiment, candidate) |> 
  arrange(total_sentiment) 


datatable(top_50_negative_bigrams,
          caption = "Top 50 Negative Phrases",
          options = list(pageLength = 20)) 



```

# ---------------------------

# Examine Keywords, Phrases in Context.

# ---------------------------

### Process Text Corpus Object

```{r}

# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
docvars(my_corpus)$candidate <- combined$candidate 
```

### Search for the term "media"

```{r}
# First add a unique identifier to combined
combined <- combined |>
  mutate(doc_id = paste0(filename, "_", row_number()))

# First, create a lookup dataframe from your original combined data
lookup_df <- combined |>
  select(doc_id, candidate) |>
  distinct()

my_tokens <- tokens(my_corpus)

# Now modify your KWIC analysis to join with this lookup
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |> 
  as.data.frame() |>
  # Extract the filename part before the underscore and row number
  mutate(doc_id = docname) |>
  left_join(lookup_df, by = "doc_id")

```

### Search Journalism Terms

```{r}
media_narratives <- kwic(my_tokens, 
                       pattern = c("media", "reporter", "newspaper", "journalist"), 
                       valuetype = "fixed") |> 
  as.data.frame() |>
  mutate(doc_id = docname) |>
  left_join(lookup_df, by = "doc_id")
```

```{r}
media_narratives_table <- media_narratives |> 
  select(candidate, pre, keyword, post, doc_id)

datatable(media_narratives_table,
          caption = "Top 50 Negative Phrases",
          options = list(pageLength = 20)) 
```

# ---------------------------

# Extracting adjectives, adverbs

# ---------------------------

**From Claude.ai**
This code will:

Install and load necessary packages
Download and load the English language model for POS tagging
Annotate your text with POS tags
Extract only the adjectives
Create two summaries:

Overall frequency count of adjectives across all articles
Frequency count of adjectives by article



The output will show you:

The adjective word (token)
How many times it appears total (frequency)
In how many different articles it appears (articles)



## Dependency parsing
```{r}
# Install and load required packages
if (!require(udpipe)) install.packages("udpipe")
if (!require(dplyr)) install.packages("dplyr")
library(udpipe)
library(tidyverse)
```

```{r}

# Download and load the English language model (only need to do this once)
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
```


```{r}
# Process the text and extract adjectives
# This will create a data frame with POS tags
annotated_text_kh <- udpipe_annotate(udmodel, x = harris_articles_df$sentence, doc_id = harris_articles_df$filename)
annotated_df_kh <- as.data.frame(annotated_text_kh)
```


```{r}
# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts_kh <- annotated_df_kh %>%
  filter(upos == "ADJ") %>%
  group_by(token) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(doc_id)
  ) %>%
  arrange(desc(frequency))

# View the top 20 most frequent adjectives
head(adjective_counts_kh, 20)

datatable(adjective_counts_kh,
          caption = "Top adjectives in Harris DW coverage",
          options = list(pageLength = 50)) 
# Optional: Save results to CSV
#write.csv(adjective_counts, "adjective_frequencies.csv", row.names = FALSE)
```

```{r}
# If you want to see adjectives by article
adjectives_by_article_kh <- annotated_df_kh %>%
  filter(upos == "ADJ") %>%
  group_by(doc_id, token) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(doc_id, desc(frequency))


```

### Extract adjective-noun pairs

**From claude.ai**
This modified version will:

Extract adjective-noun pairs where the adjective directly modifies the noun (amod dependency relation)
Create a summary showing:

The adjective-noun pair
How many times this pair appears (frequency)
In how many different articles the pair appears (articles)


Optionally show the distribution of pairs by article

The output will show combinations like "big house", "red car", "important issue", etc., along with their frequencies. Note that this will only catch direct adjectival modifications - more complex relationships might need additional parsing rules.


```{r}
adj_noun_pairs_kh <- annotated_df_kh %>%
  # Join the dataframe with itself to connect adjectives to their head words
  inner_join(
    annotated_df_kh %>% select(doc_id, sentence_id, token, token_id, upos),
    by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
  ) %>%
  # Filter for adjectives modifying nouns
  filter(
    dep_rel == "amod" &    # amod = adjectival modifier
    upos.x == "ADJ" &      # first token is adjective
    upos.y == "NOUN"       # head word is noun
  ) %>%
  # Select and rename relevant columns
  select(
    article_id = doc_id,
    sentence_id,
    adjective = token.x,
    noun = token.y
  )

# Count frequencies of adjective-noun pairs
adj_noun_counts_kh <- adj_noun_pairs_kh %>%
  group_by(adjective, noun) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(article_id)
  ) %>%
  arrange(desc(frequency))

datatable(adj_noun_counts_kh,
          caption = "Top adjectives-noun pairs in Harris DW coverage",
          options = list(pageLength = 50)) 

```



```{r}


# Optional: Save results to CSV
write.csv(adj_noun_counts_kh, "adjective_noun_pairs_kh.csv", row.names = FALSE)

# If you want to see pairs by article
pairs_by_article_kh <- adj_noun_pairs_kh %>%
  group_by(article_id, adjective, noun) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(article_id, desc(frequency))
```




Analyzing adjective in the Daily Wire articles about Donald Trump 

```{r}

# Process the text and extract adjectives
# This will create a data frame with POS tags
annotated_text_dt <- udpipe_annotate(udmodel, x = trump_articles_df$sentence, doc_id = trump_articles_df$filename)
annotated_df_dt <- as.data.frame(annotated_text_dt)

```

```{r}

# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts_dt <- annotated_df_dt %>%
  filter(upos == "ADJ") %>%
  group_by(token) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(doc_id)
  ) %>%
  arrange(desc(frequency))

# View the top 20 most frequent adjectives
head(adjective_counts_dt, 20)

datatable(adjective_counts_dt,
          caption = "Top adjectives in Harris DW coverage",
          options = list(pageLength = 50)) 
# Optional: Save results to CSV
#write.csv(adjective_counts, "adjective_frequencies.csv", row.names = FALSE)



```

```{r}
# If you want to see adjectives by article
adjectives_by_article_dt <- annotated_df_dt %>%
  filter(upos == "ADJ") %>%
  group_by(doc_id, token) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(doc_id, desc(frequency))

```

```{r}

adj_noun_pairs_dt <- annotated_df_dt %>%
  # Join the dataframe with itself to connect adjectives to their head words
  inner_join(
    annotated_df_dt %>% select(doc_id, sentence_id, token, token_id, upos),
    by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
  ) %>%
  # Filter for adjectives modifying nouns
  filter(
    dep_rel == "amod" &    # amod = adjectival modifier
    upos.x == "ADJ" &      # first token is adjective
    upos.y == "NOUN"       # head word is noun
  ) %>%
  # Select and rename relevant columns
  select(
    article_id = doc_id,
    sentence_id,
    adjective = token.x,
    noun = token.y
  )

# Count frequencies of adjective-noun pairs
adj_noun_counts_dt <- adj_noun_pairs_dt %>%
  group_by(adjective, noun) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(article_id)
  ) %>%
  arrange(desc(frequency))

datatable(adj_noun_counts_dt,
          caption = "Top adjectives-noun pairs in Harris DW coverage",
          options = list(pageLength = 50)) 


```

```{r}

# Optional: Save results to CSV
write.csv(adj_noun_counts_dt, "adjective_noun_pairs_dt.csv", row.names = FALSE)

# If you want to see pairs by article
pairs_by_article_dt <- adj_noun_pairs_dt %>%
  group_by(article_id, adjective, noun) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(article_id, desc(frequency))


```



# ---------------------------

# Extra Content

# ---------------------------

### Sentiment of Trump, Harris headlines

```{r}
#tokenize the headlines only
headline_sentiment <- combined_index |> 
  select(filename, candidate, index) |>  
  mutate(text = str_replace_all(filename, "_", " "),
         text = str_replace_all(text, ".txt", "")) |>    group_by(index, candidate) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
headline_sentiment_analysis <- headline_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index, candidate) |>  
  summarize(sentiment = sum(value), .groups = "drop") |> 
  inner_join(combined_index, by = c("index", "candidate"))

# aggregate at article level, total sentiment score (Positive or Negative)
headline_sentiment_analysis <- headline_sentiment_analysis|>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) 

```

### Headline sentiment charted

```{r}

ggplot(headline_sentiment_analysis, 
       aes(x = date, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
  labs(title = "Harris, Trump Headline Sentiment in Daily Wire coverage",
       caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
```

### Building the Trump text dataframe

```{r}

tfiles <- list.files("./trump_extracted_text", pattern="*.txt") |> 
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(name = str_replace_all(filename, ".txt", "")) |> 
  mutate(name = str_replace_all(tolower(name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number())


trump_index <- rio::import("DW_Trump_Articles.xlsx") |> 
  mutate(name = str_replace_all(tolower(Name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number()) |> 
  rename(date = N) |> 
  distinct(name, .keep_all = TRUE)
```

```{r}
tfinal_index <- tfiles |> 
  inner_join(trump_index, by=c("name")) |> 
  mutate(filepath = paste0("./trump_extracted_text/", filename)) |> 
  janitor::clean_names() |> 
  rename(index = index_x) |> 
  distinct(index, .keep_all = TRUE) |> 
  select(index, date,filename,url, filepath, filename)

#fact check
anti <- tfiles |> 
  anti_join(trump_index, by=c("name"))

```

Text compiler

```{r include=FALSE, echo=FALSE}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- tfinal_index |>
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) |>
    as_tibble() |>
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df |>
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(tfinal_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

trump_articles_df <- articles_df |>
  select(filename, sentence=value) |>
  inner_join(tfinal_index)

```

clean data

```{r}
phrases <- c("Date:" = "", 
            "Article Text:" = "",
            "Headline:" = "",
            "Already have an account?" = "", 
            "Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
            "Stay up-to-date on the latestnews, podcasts, and more." = "",
  "https" = "")


trump_articles_df <- trump_articles_df |> 
  mutate(sentence = str_replace_all(sentence, phrases))

#write.csv(trump_articles_df, "trump_extracted_text_jan2025.csv")
```

Trump Bigrams

```{r}
#library(tidytext)
trump_bigrams <- trump_articles_df |> 
  select(sentence) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |> 
  filter(!is.na(word1))

```

Top 20 bigrams

```{r}

top_20_trump_bigrams <- trump_bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```

## Chart Trump Bigrams

```{r}
ggplot(top_20_trump_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Donald Trump on Daily Wire coverage",
       caption = "n=99 articles, 2024. Graphic by Rob Wells. 1-23-2025",
       x = "Phrase",
       y = "Count of terms")
```

## Trump Sentiment analysis

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df |> 
  select(sentence, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
trump_sentiment_analysis <- trump_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index) |>  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |> 
  inner_join(tfinal_index, by=("index")) |> 
  select(index, date, sentiment, sentiment_type, filename, url)
```

Chart sentiment by article

```{r}

ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
       caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Articles",
       y = "Sentiment Score") 
```



## Combine Harris, Trump articles

```{r}
harris_articles_df <- harris_articles_df |> 
  mutate(candidate = "harris") 

trump_articles_df <- trump_articles_df |> 
  mutate(candidate = "trump")

#combined <- rbind(harris_articles_df, trump_articles_df)

#write.csv(combined, "trump-harris-DW-combined.csv")

```
