---
title: "Nini Text Analysis"
author: "Rob Wells"
date: "2025-02-05"
output: html_document
---

Independent study with Nini Mtchedlishvili

This code demonstrates text processing, cleaning and the creation of bigrams and sentiment analysis from a group of Daily Wire news articles about Kamala Harris and Donald Trump.

**Credits:**

-   Julia Silge & David Robinson, Text Mining with R: <https://www.tidytextmining.com/>

-   Nino Mtchedlishvili, Ph.D. student at the Philip Merrill College of Journalism, contributed to this project.

### Load software

```{r message=FALSE, warning=FALSE}
#Install packages if you don't have them already
#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("DT")

library(tidyverse)
library(tidytext) 
library(quanteda)
library(readtext)
library(DT)

```

### Harris text

```{r}

files <- list.files("./kamala_extracted_text", pattern="*.txt") |> 
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(name = str_replace_all(filename, ".txt", "")) |> 
  mutate(name = str_replace_all(tolower(name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number())
         
          
 #  #create an index with the file name
 # mutate(index = str_extract(filename, "\\d+")) |> 
 #  mutate(index = as.numeric(index))

kharris_index <- rio::import("DW_Kamala_Articles.xlsx") |> 
  mutate(name = str_replace_all(tolower(NAME), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number()) |> 
  distinct(name, .keep_all = TRUE) |> 
  mutate(DATE = case_when(
    str_detect(as.character(DATE), "2023-11-04") ~ as_datetime("2024-11-04"),
    TRUE ~ DATE
    ))
```

```{r}
final_index <- files |> 
  inner_join(kharris_index, by=c("name")) |> 
  mutate(filepath = paste0("./kamala_extracted_text/", filename)) |> 
  janitor::clean_names() |> 
  rename(index = index_x) |> 
  distinct(index, .keep_all = TRUE) |> 
  select(index, date,filename,url, filepath, filename)

#fact check
anti <- files |> 
  anti_join(kharris_index, by=c("name"))

```

**don't need to run this skip to line 163**
### Text compiler

```{r include=FALSE, echo=FALSE}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index |>
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) |>
    as_tibble() |>
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df |>
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df |>
  select(filename, sentence=value) |>
  inner_join(final_index)

```

## Clean data

```{r}
phrases <- c("Date:" = "", 
            "Article Text:" = "",
            "Headline:" = "",
            "Already have an account?" = "", 
            "Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
            "Stay up-to-date on the latestnews, podcasts, and more." = "",
  "https" = "")


harris_articles_df <- articles_df |> 
   mutate(sentence = str_replace_all(sentence, phrases))

#write.csv(harris_articles_df, "kharris_extracted_text_jan2025.csv")
```

# ---------------------------

# Load harris_articles_df data

# ---------------------------

```{r}
harris_articles_df <- read.csv("kharris_extracted_text_jan2025.csv")
```

fact check

```{r}
harris_articles_df |> 
  distinct(filename) |> 
  count(filename) |> 
  summarise(sum(n))

```

### Harris Bigrams - Top Phrases

```{r}
harris_bigrams <- harris_articles_df |> 
  select(sentence) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |> 
  filter(!is.na(word1))

head(harris_bigrams)
```

```{r include=FALSE}

top_20_harris_bigrams <- harris_bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```

```{r}
datatable(top_20_harris_bigrams,
          caption = "Top 20 Harris Phrases",
          options = list(pageLength = 20)) 
```

## Chart Top Harris Phrases

```{r}
ggplot(top_20_harris_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Kamala Harris on daily Wire coverage",
       caption = "n=94 articles, 2024. Graphic by Rob Wells. 1-23-2025",
       x = "Phrase",
       y = "Count of terms")
```

### Measure Sentiment of Harris articles

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
harris_sentiment <- harris_articles_df |> 
  select(sentence, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# join tokenized words with the sentiment dictionary
harris_sentiment_analysis <- harris_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index) |>  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |> 
  inner_join(final_index, by=("index")) |> 
  select(index, date, sentiment, sentiment_type, filename, url)
```

## Chart sentiment by article

```{r warning=FALSE}

ggplot(harris_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of Daily Wire coverage of Kamala Harris, October 2024",
       caption = "n=96 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Articles",
       y = "Sentiment Score") 
```

# ---------------------------

# Analyze Trump and Harris Articles

# ---------------------------

```{r}
combined <- read.csv("trump-harris-DW-combined.csv")

#There are nine common articles the Trump / Harris corpra. Create a new category for shared article
combined_index <- combined |>
  distinct(filename, url, candidate, .keep_all = TRUE) |>
  group_by(filename) |>
  mutate(shared_article = n() > 1) |>
  ungroup()

#write.csv(combined_index, "combined_index.csv")

```

fact check

```{r}
combined |> 
  distinct(filename, url, candidate) |>
  count(candidate)
```

### Sentiment of Daily News coverage

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
combined_sentiment <- combined |> 
  select(sentence, candidate, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index, candidate) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
combined_sentiment_analysis <- combined_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index, candidate) |>  
  summarize(sentiment = sum(value), .groups = "drop") |> 
  inner_join(combined_index, by = c("index", "candidate"))

# aggregate at article level, total sentiment score (Positive or Negative)
combined_sentiment_analysis <- combined_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) 

```

### Chart Trump, Harris Sentiment by Date

```{r}

ggplot(combined_sentiment_analysis, 
       aes(x = date, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
  labs(title = "Harris, Trump Sentiment in Daily Wire coverage",
       caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
```

# Examine Top Negative Phrases

```{r}
negative_bigrams <- combined |> 
  select(sentence, candidate) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  #adding candidate here to specify Trump v Harris
  count(candidate, word1, word2, sort = TRUE) |> 
  filter(!is.na(word1)) |> 
  mutate(bigram = paste0(word1," ", word2))|> 
  #left join keeps all scores, need to do it twice
   left_join(afinn, by = c("word1" = "word")) |>
  rename(value1 = value) |>
  # left join second word
  left_join(afinn, by = c("word2" = "word")) |>
  rename(value2 = value) |>
  # Combine scores
  mutate(
    total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
  ) |> 
  filter(total_sentiment < 0)
  
```

```{r}
top_50_negative_bigrams <- negative_bigrams |> 
  slice_min(total_sentiment, n= 50) |> 
  select(bigram, total_sentiment, candidate) |> 
  arrange(total_sentiment) 


datatable(top_50_negative_bigrams,
          caption = "Top 50 Negative Phrases",
          options = list(pageLength = 20)) 



```

# ---------------------------

# Examine Keywords, Phrases in Context.

# ---------------------------

### Process Text Corpus Object

```{r}

# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
docvars(my_corpus)$candidate <- combined$candidate 
```

### Search for the term "media"

```{r}
# First add a unique identifier to combined
combined <- combined |>
  mutate(doc_id = paste0(filename, "_", row_number()))

# First, create a lookup dataframe from your original combined data
lookup_df <- combined |>
  select(doc_id, candidate) |>
  distinct()

my_tokens <- tokens(my_corpus)

# Now modify your KWIC analysis to join with this lookup
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |> 
  as.data.frame() |>
  # Extract the filename part before the underscore and row number
  mutate(doc_id = docname) |>
  left_join(lookup_df, by = "doc_id")

```

### Search Journalism Terms

```{r}
media_narratives <- kwic(my_tokens, 
                       pattern = c("media", "reporter", "newspaper", "journalist"), 
                       valuetype = "fixed") |> 
  as.data.frame() |>
  mutate(doc_id = docname) |>
  left_join(lookup_df, by = "doc_id")
```

```{r}
media_narratives_table <- media_narratives |> 
  select(candidate, pre, keyword, post, doc_id)

datatable(media_narratives_table,
          caption = "Top 50 Negative Phrases",
          options = list(pageLength = 20)) 
```

# ---------------------------

# Extracting adjectives, adverbs

# ---------------------------

**From Claude.ai**
This code will:

Install and load necessary packages
Download and load the English language model for POS tagging
Annotate your text with POS tags
Extract only the adjectives
Create two summaries:

Overall frequency count of adjectives across all articles
Frequency count of adjectives by article



The output will show you:

The adjective word (token)
How many times it appears total (frequency)
In how many different articles it appears (articles)



## Dependency parsing
```{r}
# Install and load required packages
if (!require(udpipe)) install.packages("udpipe")
if (!require(dplyr)) install.packages("dplyr")
library(udpipe)
library(tidyverse)
```

```{r}

# Download and load the English language model (only need to do this once)
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
```


```{r}
# Process the text and extract adjectives
# This will create a data frame with POS tags
annotated_text <- udpipe_annotate(udmodel, x = harris_articles_df$sentence, doc_id = harris_articles_df$filename)
annotated_df <- as.data.frame(annotated_text)
```


```{r}
# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts <- annotated_df %>%
  filter(upos == "ADJ") %>%
  group_by(token) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(doc_id)
  ) %>%
  arrange(desc(frequency))

# View the top 20 most frequent adjectives
head(adjective_counts, 20)

datatable(adjective_counts,
          caption = "Top adjectives in Harris DW coverage",
          options = list(pageLength = 50)) 
# Optional: Save results to CSV
#write.csv(adjective_counts, "adjective_frequencies.csv", row.names = FALSE)
```

```{r}
# If you want to see adjectives by article
adjectives_by_article <- annotated_df %>%
  filter(upos == "ADJ") %>%
  group_by(doc_id, token) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(doc_id, desc(frequency))


```

### Extract adjective-noun pairs

**From claude.ai**
This modified version will:

Extract adjective-noun pairs where the adjective directly modifies the noun (amod dependency relation)
Create a summary showing:

The adjective-noun pair
How many times this pair appears (frequency)
In how many different articles the pair appears (articles)


Optionally show the distribution of pairs by article

The output will show combinations like "big house", "red car", "important issue", etc., along with their frequencies. Note that this will only catch direct adjectival modifications - more complex relationships might need additional parsing rules.


```{r}
adj_noun_pairs <- annotated_df %>%
  # Join the dataframe with itself to connect adjectives to their head words
  inner_join(
    annotated_df %>% select(doc_id, sentence_id, token, token_id, upos),
    by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
  ) %>%
  # Filter for adjectives modifying nouns
  filter(
    dep_rel == "amod" &    # amod = adjectival modifier
    upos.x == "ADJ" &      # first token is adjective
    upos.y == "NOUN"       # head word is noun
  ) %>%
  # Select and rename relevant columns
  select(
    article_id = doc_id,
    sentence_id,
    adjective = token.x,
    noun = token.y
  )

# Count frequencies of adjective-noun pairs
adj_noun_counts <- adj_noun_pairs %>%
  group_by(adjective, noun) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(article_id)
  ) %>%
  arrange(desc(frequency))

datatable(adj_noun_counts,
          caption = "Top adjectives-noun pairs in Harris DW coverage",
          options = list(pageLength = 50)) 

```



```{r}


# Optional: Save results to CSV
write.csv(adj_noun_counts, "adjective_noun_pairs.csv", row.names = FALSE)

# If you want to see pairs by article
pairs_by_article <- adj_noun_pairs %>%
  group_by(article_id, adjective, noun) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(article_id, desc(frequency))
```


# ---------------------------

# Extra Content

# ---------------------------

### Sentiment of Trump, Harris headlines

```{r}
#tokenize the headlines only
headline_sentiment <- combined_index |> 
  select(filename, candidate, index) |>  
  mutate(text = str_replace_all(filename, "_", " "),
         text = str_replace_all(text, ".txt", "")) |>    group_by(index, candidate) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
headline_sentiment_analysis <- headline_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index, candidate) |>  
  summarize(sentiment = sum(value), .groups = "drop") |> 
  inner_join(combined_index, by = c("index", "candidate"))

# aggregate at article level, total sentiment score (Positive or Negative)
headline_sentiment_analysis <- headline_sentiment_analysis|>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) 

```

### Headline sentiment charted

```{r}

ggplot(headline_sentiment_analysis, 
       aes(x = date, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
  labs(title = "Harris, Trump Headline Sentiment in Daily Wire coverage",
       caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
```

### Building the Trump text dataframe

```{r}

tfiles <- list.files("./trump_extracted_text", pattern="*.txt") |> 
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(name = str_replace_all(filename, ".txt", "")) |> 
  mutate(name = str_replace_all(tolower(name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number())


trump_index <- rio::import("DW_Trump_Articles.xlsx") |> 
  mutate(name = str_replace_all(tolower(Name), " ", "_")) |> 
  mutate(name = str_replace_all(name, "[[:punct:]]", "")) |> 
  arrange((name)) |> 
  mutate(index = row_number()) |> 
  rename(date = N) |> 
  distinct(name, .keep_all = TRUE)
```

```{r}
tfinal_index <- tfiles |> 
  inner_join(trump_index, by=c("name")) |> 
  mutate(filepath = paste0("./trump_extracted_text/", filename)) |> 
  janitor::clean_names() |> 
  rename(index = index_x) |> 
  distinct(index, .keep_all = TRUE) |> 
  select(index, date,filename,url, filepath, filename)

#fact check
anti <- tfiles |> 
  anti_join(trump_index, by=c("name"))

```

Text compiler

```{r include=FALSE, echo=FALSE}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- tfinal_index |>
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) |>
    as_tibble() |>
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df |>
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(tfinal_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

trump_articles_df <- articles_df |>
  select(filename, sentence=value) |>
  inner_join(tfinal_index)

```

clean data

```{r}
phrases <- c("Date:" = "", 
            "Article Text:" = "",
            "Headline:" = "",
            "Already have an account?" = "", 
            "Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
            "Stay up-to-date on the latestnews, podcasts, and more." = "",
  "https" = "")


trump_articles_df <- trump_articles_df |> 
  mutate(sentence = str_replace_all(sentence, phrases))

#write.csv(trump_articles_df, "trump_extracted_text_jan2025.csv")
```

Trump Bigrams

```{r}
#library(tidytext)
trump_bigrams <- trump_articles_df |> 
  select(sentence) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |> 
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |> 
  filter(!is.na(word1))

```

Top 20 bigrams

```{r}

top_20_trump_bigrams <- trump_bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```

## Chart Trump Bigrams

```{r}
ggplot(top_20_trump_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Donald Trump on Daily Wire coverage",
       caption = "n=99 articles, 2024. Graphic by Rob Wells. 1-23-2025",
       x = "Phrase",
       y = "Count of terms")
```

## Trump Sentiment analysis

```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df |> 
  select(sentence, index) |> 
  mutate(text = str_squish(sentence)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(index) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
trump_sentiment_analysis <- trump_sentiment |>
  inner_join(afinn, by = c("tokens"="word")) |>
  group_by(index) |>  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis |>
   group_by(index) |> 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |> 
  inner_join(tfinal_index, by=("index")) |> 
  select(index, date, sentiment, sentiment_type, filename, url)
```

Chart sentiment by article

```{r}

ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
       caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
       x = "Articles",
       y = "Sentiment Score") 
```

## Combine Harris, Trump articles

```{r}
harris_articles_df <- harris_articles_df |> 
  mutate(candidate = "harris") 

trump_articles_df <- trump_articles_df |> 
  mutate(candidate = "trump")

#combined <- rbind(harris_articles_df, trump_articles_df)

#write.csv(combined, "trump-harris-DW-combined.csv")

```
